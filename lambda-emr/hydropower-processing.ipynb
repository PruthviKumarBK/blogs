{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql.session import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import Row, Column\n",
    "from pyspark.sql import functions as f\n",
    "from dateutil import relativedelta\n",
    "from datetime import timedelta\n",
    "import argparse\n",
    "import boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(JOB_DATE=None, REGION=None, S3_BUCKET=None)"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--JOB_DATE', dest='JOB_DATE')\n",
    "parser.add_argument('--S3_BUCKET', dest='S3_BUCKET')\n",
    "parser.add_argument('--REGION', dest='REGION')\n",
    "args = parser.parse_args()\n",
    "print(args)\n",
    "JOB_DATE=args.JOB_DATE\n",
    "S3_BUCKET=args.S3_BUCKET\n",
    "REGION=args.REGION\n",
    "\n",
    "READ_PATH='data/'+JOB_DATE\n",
    "S3_READ_PATH='s3://'+S3_BUCKET+'/'+READ_PATH\n",
    "WRITE_PATH='curated/'+JOB_DATE\n",
    "S3_WRITE_PATH='s3://'+S3_BUCKET+'/'+WRITE_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sc = SparkContext.getOrCreate()\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def does_s3key_exist(bucket, key, ext):\n",
    "    s3 = boto3.resource('s3')\n",
    "    bucket = s3.Bucket(bucket)\n",
    "    objects = bucket.objects.all()\n",
    "    FOUND=0\n",
    "    for object in objects:\n",
    "        if object.key.startswith(key) and object.key.endswith(ext):\n",
    "            FOUND=1\n",
    "    return FOUND"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+----+-----------+\n",
      "|Entity|Code|Year|consumption|\n",
      "+------+----+----+-----------+\n",
      "|Africa|null|1965|  14.278806|\n",
      "|Africa|null|1966|  15.649049|\n",
      "|Africa|null|1967|  16.158333|\n",
      "|Africa|null|1968|  18.622982|\n",
      "|Africa|null|1969|  21.582897|\n",
      "+------+----+----+-----------+\n",
      "only showing top 5 rows"
     ]
    }
   ],
   "source": [
    "if does_s3key_exist(S3_BUCKET, READ_PATH, '.csv') == 1:\n",
    "    hydropower_consumption_df=spark.read.csv(S3_READ_PATH, header=True)\n",
    "    hydropower_consumption_df=hydropower_consumption_df.withColumnRenamed(\"Hydropower (Terawatt-hours)\",'consumption') \\\n",
    "                                                   .withColumn('Year', f.col('Year').cast(IntegerType())) \\\n",
    "                                                   .withColumn('consumption', f.col('consumption').cast(FloatType()))\n",
    "    hydropower_consumption_df.show(5)\n",
    "    hydropower_consumption_df.write.parquet(S3_WRITE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Sparkmagic (PySpark)",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
